{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lochieh19798/Data-analysis/blob/main/Figure2mod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edition control"
      ],
      "metadata": {
        "id": "Soz1YsEaL0XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"lochieh19798\"\n",
        "!git config --global user.email \"lochieh19798@gmail.com\""
      ],
      "metadata": {
        "id": "2M1BW0t8Lz5d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lochieh19798/data-analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrOB_TJxMC9j",
        "outputId": "9f15495d-3061-4b83-820c-869ebb78c3a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'data-analysis' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCXII-AEgAjH",
        "outputId": "9f8773b5-e08e-4580-e2c2-4165d1f0549e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ╔═╡ Cell 0 ═════════════════════════════════════════════════════════════╗\n",
        "# If you’re on Colab, install any non-preinstalled libraries:\n",
        "!pip install -q catboost shap scikit-learn==1.4.2 matplotlib pandas numpy\n",
        "!pip install -q --upgrade xgboost\n",
        "!pip install -q shap --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUmxufPcmQEf",
        "outputId": "d0c38202-af21-4b91-b9db-1366f69b33d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched XGBModel.feature_weights → None\n"
          ]
        }
      ],
      "source": [
        "from xgboost.sklearn import XGBModel\n",
        "XGBModel.feature_weights = None\n",
        "print(\"Patched XGBModel.feature_weights →\", XGBModel.feature_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lplcCkEkje8m"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 1 ═════════════════════════════════════════════════════════════╗\n",
        "# Imports & global configuration\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import (\n",
        "    OneHotEncoder,\n",
        "    StandardScaler,\n",
        "    OrdinalEncoder\n",
        ")\n",
        "from sklearn.impute import (\n",
        "    KNNImputer,\n",
        "    SimpleImputer\n",
        "    # ← no IterativeImputer here\n",
        ")\n",
        "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    HistGradientBoostingClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "plt.style.use(\"default\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AWh_G5BjgON",
        "outputId": "ca2c17e6-789d-474c-c849-52020df9599e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Missing Value Percentage per Feature:\n",
            "                                                                                          variable  missing_pct\n",
            "                                                                                      Baseline LAD        32.7%\n",
            "                                                                                   Change in EQ 5D        27.0%\n",
            "                                                            Anxiety/Depression question (Baseline)        25.6%\n",
            "                                                               Pain/Discomfort question (Baseline)        25.6%\n",
            "                                                              Usual activities question (Baseline)        25.6%\n",
            "                                                                     Self-care question (Baseline)        25.6%\n",
            "                                     Visual analogue score: Your own health state today (Baseline)        25.6%\n",
            "                                                                      Mobility question (Baseline)        25.6%\n",
            "                                                                          Coldest Temperature LIPV        25.3%\n",
            "                                                                                     Baseline LVEF        23.8%\n",
            "                                                                          Coldest temperature RSPV        23.8%\n",
            "                                                                          Coldest Temperature LSPV        23.8%\n",
            "                                                                          Coldest Temperature RIPV        23.8%\n",
            "                                                                                 AF_time_procedure         9.6%\n",
            "Left atrial dwell time: time from first cryocatheter insertion to last cryocatheter removal (mins)         7.8%\n",
            "                                                                          Total fluoro time (mins)         7.5%\n",
            "                                                                              Energy duration LIPV         4.3%\n",
            "                                                    Were all targeted PVs isolated (Investigator)?         4.3%\n",
            "                                       Subject taking Class I or III AAD at baseline (1=Yes, 0=No)         2.8%\n",
            "                                                                              Energy duration LSPV         2.5%\n",
            "                                                                              Energy duration RSPV         2.1%\n",
            "                                                                              Energy duration RIPV         2.1%\n",
            "                                                                                  LIPV PV isolated         1.1%\n",
            "                                                                                 Pre Procedural CT         0.7%\n",
            "                                                                                     RIPV isolated         0.7%\n",
            "                                                                                     LSPV isolated         0.7%\n",
            "                                                                                     RSPV Isolated         0.7%\n",
            "                                                                                 Ensite 3D mapping         0.4%\n",
            "                                                                                               BMI         0.4%\n",
            "                                                                                               age         0.0%\n",
            "                                                                                       CHAD2 score         0.0%\n",
            "                            Total procedure time: Venous access to last cryoatheter removal (mins)         0.0%\n",
            "                                                           Total no of ablation application number         0.0%\n",
            "                                                                                 CHA2DS2VASc score         0.0%\n",
            "                                                                                               CAD         0.0%\n",
            "                                                                                    Sex (F=1, M=0)         0.0%\n",
            "                                                                                                HF         0.0%\n",
            "                                                                                          Diabetes         0.0%\n",
            "                                                                                      Hypertension         0.0%\n",
            "                                                                                    History of TIA         0.0%\n",
            "                                                                                            stroke         0.0%\n",
            "                                                                                          AF_Parox         0.0%\n",
            "                                                                                        AF_Persist         0.0%\n",
            "                                                                        Non-PVI ablation performed         0.0%\n",
            "                                                                                      CTI ablation         0.0%\n",
            "                                   Mapping/navigational tools: Intracardiac echocardiography (ICE)         0.0%\n",
            "                               Was subject taking Class I or Class III AAD at procedure discharge?         0.0%\n"
          ]
        }
      ],
      "source": [
        "# ╔═╡ Cell 2 – load data & build feature lists with missing‐% report ═════╝\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Data‐loading helper (as before)\n",
        "DATA_PATH = \"/content/data-analysis/CRYOANALYSIS.csv\"\n",
        "def load_and_prepare_data(csv_path: str = DATA_PATH) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # (1) Recurrence flags\n",
        "    df[\"Survival_time\"] = pd.to_numeric(df[\"Survival_time\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"Survival_time\", \"Recurrence\"])\n",
        "    df[\"Recurrence_1yr\"] = ((df[\"Survival_time\"] <= 365) & (df[\"Recurrence\"] == 1)).astype(int)\n",
        "    df[\"Recurrence_2yr\"] = ((df[\"Survival_time\"] <= 730) & (df[\"Recurrence\"] == 1)).astype(int)\n",
        "    # (2) AF type one‐hot\n",
        "    af_col = \"Baseline AF Type(1=paroxysmal, 2=persistent)\"\n",
        "    df[\"AF_Parox\"]   = (df[af_col] == 1).astype(int)\n",
        "    df[\"AF_Persist\"] = (df[af_col] == 2).astype(int)\n",
        "    return df\n",
        "\n",
        "# 2. Load the dataset\n",
        "DATA_PATH = Path(\"/content/data-analysis/CRYOANALYSIS.csv\")\n",
        "assert DATA_PATH.exists(), f\"{DATA_PATH} not found – upload it first!\"\n",
        "df = load_and_prepare_data(DATA_PATH)\n",
        "\n",
        "# 3. Feature lists\n",
        "cont = [\n",
        "    \"age\", \"BMI\",\n",
        "    \"Baseline LVEF\", \"Baseline LAD\", \"CHA2DS2VASc score\", \"CHAD2 score\", \"AF_time_procedure\",\n",
        "    \"Total no of ablation application number\", \"Mobility question (Baseline)\",\n",
        "    \"Self-care question (Baseline)\", \"Usual activities question (Baseline)\",\n",
        "    \"Pain/Discomfort question (Baseline)\", \"Anxiety/Depression question (Baseline)\",\n",
        "    \"Visual analogue score: Your own health state today (Baseline)\",\n",
        "    \"Total procedure time: Venous access to last cryoatheter removal (mins)\",\n",
        "    \"Total fluoro time (mins)\", \"Energy duration LSPV\", \"Coldest Temperature LSPV\",\n",
        "    \"Energy duration LIPV\", \"Coldest Temperature LIPV\",\n",
        "    \"Energy duration RSPV\", \"Coldest temperature RSPV\",\n",
        "    \"Energy duration RIPV\", \"Coldest Temperature RIPV\",\n",
        "    \"Left atrial dwell time: time from first cryocatheter insertion to last cryocatheter removal (mins)\",\n",
        "    \"Change in EQ 5D\"\n",
        "    # \"Time to isolation LSPV\", \"Time to isolation LIPV\", \"Time to isolation RSPV\",\"Time to isolation RIPV\",\n",
        "]\n",
        "\n",
        "cat = [\n",
        "    \"Sex (F=1, M=0)\", \"Hypertension\", \"Diabetes\", \"HF\", \"CAD\", \"stroke\",\n",
        "    \"History of TIA\", \"Subject taking Class I or III AAD at baseline (1=Yes, 0=No)\",\n",
        "    \"LSPV isolated\", \"LIPV PV isolated\", \"RSPV Isolated\", \"RIPV isolated\",\n",
        "    \"Were all targeted PVs isolated (Investigator)?\",\n",
        "    \"AF_Parox\", \"AF_Persist\",\n",
        "    \"CTI ablation\",\n",
        "    \"Non-PVI ablation performed\",\n",
        "    \"Was subject taking Class I or Class III AAD at procedure discharge?\",\n",
        "    \"Mapping/navigational tools: Intracardiac echocardiography (ICE)\",\n",
        "    \"Pre Procedural CT\",\n",
        "    \"Ensite 3D mapping\"\n",
        "]\n",
        "\n",
        "TARGET = \"Recurrence_2yr\"\n",
        "\n",
        "# 4. Report missing‐value percentages for every feature\n",
        "all_feats = cont + cat\n",
        "missing_pct = df[all_feats].isna().mean() * 100\n",
        "missing_report = (\n",
        "    missing_pct\n",
        "      .sort_values(ascending=False)\n",
        "      .rename(\"missing_pct\")\n",
        "      .reset_index()\n",
        "      .rename(columns={\"index\": \"variable\"})\n",
        ")\n",
        "print(\"📊 Missing Value Percentage per Feature:\")\n",
        "print(missing_report.to_string(index=False, float_format=\"%.1f%%\"))\n",
        "\n",
        "# 5. Gentle column check: warn & drop if still missing\n",
        "missing_cont = [c for c in cont if c not in df.columns]\n",
        "missing_cat  = [c for c in cat  if c not in df.columns]\n",
        "if missing_cont or missing_cat:\n",
        "    print(\"\\n⚠️  WARNING – some expected columns are missing and will be skipped.\")\n",
        "    print(\"   Missing continuous :\", missing_cont)\n",
        "    print(\"   Missing categorical:\", missing_cat)\n",
        "\n",
        "cont = [c for c in cont if c in df.columns]\n",
        "cat  = [c for c in cat  if c in df.columns]\n",
        "\n",
        "# From here on, the rest of the notebook runs as before...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVMwLeDVji9q",
        "outputId": "fe22d124-6f54-442e-b249-dc8c72cc24c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Chosen cutoff at 95.0% of data (row 266 / 281)\n",
            "   • Overall event rate:  19.22%\n",
            "   • Train event rate:    19.17%\n",
            "   • Test event rate:     20.00%\n",
            "   • Train/Test sizes:    266 / 15\n"
          ]
        }
      ],
      "source": [
        "# ╔═╡ Cell 3 – event‐rate–balanced temporal hold-out ═════════════════╗\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# 1) Ensure date column and sort\n",
        "df[\"ProcedureDate\"] = pd.to_datetime(df[\"ProcedureDate\"])\n",
        "df_sorted = df.sort_values(\"ProcedureDate\")\n",
        "\n",
        "# 2) Compute overall event rate\n",
        "full_rate = df_sorted[TARGET].mean()\n",
        "\n",
        "# 3) Search for the best cutoff fraction (between 60% and 95% of the data)\n",
        "n = len(df_sorted)\n",
        "best_diff = np.inf\n",
        "best_idx = None\n",
        "best_frac = None\n",
        "\n",
        "for frac in np.linspace(0.60, 0.95, 36):            # try 0.60, 0.61, …, 0.95\n",
        "    idx = int(n * frac)\n",
        "    test_slice = df_sorted.iloc[idx:]\n",
        "    test_rate = test_slice[TARGET].mean()\n",
        "    diff = abs(test_rate - full_rate)\n",
        "    if diff < best_diff:\n",
        "        best_diff = diff\n",
        "        best_idx  = idx\n",
        "        best_frac = frac\n",
        "\n",
        "# 4) Slice into train / test\n",
        "train_df = df_sorted.iloc[:best_idx]\n",
        "test_df  = df_sorted.iloc[best_idx:]\n",
        "\n",
        "X_train = train_df[cont + cat]\n",
        "y_train = train_df[TARGET].astype(int)\n",
        "X_test  = test_df [cont + cat]\n",
        "y_test  = test_df [TARGET].astype(int)\n",
        "\n",
        "# 5) Report\n",
        "print(f\"🔎 Chosen cutoff at {best_frac*100:.1f}% of data (row {best_idx} / {n})\")\n",
        "print(f\"   • Overall event rate:  {full_rate:.2%}\")\n",
        "print(f\"   • Train event rate:    {y_train.mean():.2%}\")\n",
        "print(f\"   • Test event rate:     {y_test.mean():.2%}\")\n",
        "print(f\"   • Train/Test sizes:    {len(y_train)} / {len(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔═╡ Cell 4 – Pre-processing pipelines ═══════════════════════════════════╗\n",
        "from sklearn.pipeline           import Pipeline\n",
        "from sklearn.impute             import IterativeImputer, SimpleImputer\n",
        "from sklearn.preprocessing      import StandardScaler, PowerTransformer, OneHotEncoder\n",
        "from sklearn.compose            import ColumnTransformer\n",
        "\n",
        "# Numeric pipeline: iterative imputation → power transform → scaling\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"impute\", IterativeImputer(random_state=RANDOM_STATE)),\n",
        "    (\"power\",  PowerTransformer(method=\"yeo-johnson\")),\n",
        "    (\"scale\",  StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical pipeline: fill missing with \"Missing\" → one-hot encode\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "# Full preprocessor: apply to continuous and categorical lists\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe,       cont),\n",
        "    (\"cat\", categorical_pipe,   cat)\n",
        "], remainder=\"drop\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "blz14tHZe6mJ",
        "outputId": "dbb96fb3-5fe1-47f7-92a7-79e10de82edc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "IterativeImputer is experimental and the API might change without any deprecation cycle. To use it, you need to explicitly import enable_iterative_imputer:\nfrom sklearn.experimental import enable_iterative_imputer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8310503a0b44>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ╔═╡ Cell 4 – Pre-processing pipelines ═══════════════════════════════════╗\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m           \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mIterativeImputer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m      \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPowerTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m            \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/impute/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"IterativeImputer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;34mf\"{name} is experimental and the API might change without any \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;34m\"deprecation cycle. To use it, you need to explicitly import \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: IterativeImputer is experimental and the API might change without any deprecation cycle. To use it, you need to explicitly import enable_iterative_imputer:\nfrom sklearn.experimental import enable_iterative_imputer",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKukFYJdbqbW"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 4b – preview IterativeImputer + PowerTransformer on numeric features ═══════╗\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# (Optionally suppress FutureWarnings or ConvergenceWarnings)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# 1. Show missing counts before\n",
        "num_train = X_train[cont].copy()\n",
        "print(\"🔴 Missing before imputation (numeric features):\")\n",
        "print(num_train.isna().sum())\n",
        "\n",
        "# 2. Fit & transform with your updated numeric_pipe\n",
        "num_imputed = pd.DataFrame(\n",
        "    numeric_pipe.fit_transform(num_train),\n",
        "    columns=cont,\n",
        "    index=num_train.index\n",
        ")\n",
        "\n",
        "# 3. Show missing counts after\n",
        "print(\"\\n🟢 Missing after IterativeImputer + PowerTransformer:\")\n",
        "print(num_imputed.isna().sum())\n",
        "\n",
        "# 4. Display first few rows of the transformed DataFrame\n",
        "display(num_imputed.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QMFLyRLcF99"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 4c – preview SimpleImputer + one-hot encoding on categorical features ═══════╗\n",
        "\n",
        "# 1. Grab the raw cat columns (with NaNs)\n",
        "cat_train = X_train[cat].copy()\n",
        "print(\"🔴 Missing before imputation (categorical features):\")\n",
        "print(cat_train.isna().sum())\n",
        "\n",
        "# 2. Fit & transform via categorical_pipe\n",
        "cat_ohe_array = categorical_pipe.fit_transform(cat_train)\n",
        "\n",
        "# 3. Turn back into a DataFrame\n",
        "cat_ohe_cols = categorical_pipe.named_steps[\"onehot\"].get_feature_names_out(cat)\n",
        "cat_imputed = pd.DataFrame(cat_ohe_array, columns=cat_ohe_cols, index=cat_train.index)\n",
        "\n",
        "print(\"\\n🟢 Missing after SimpleImputer + OneHotEncoder:\")\n",
        "print(cat_imputed.isna().sum().sum(), \"total NaNs remaining\")\n",
        "\n",
        "print(\"\\n🖨️ Sample of the imputed one-hot matrix:\")\n",
        "display(cat_imputed.iloc[:, :10].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqXPfTpFZlGn"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 4d – inspect full preprocessed feature space ─══════════════╗\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# 1) Re‐build the ColumnTransformer as in Cell 4\n",
        "ct = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe, cont),\n",
        "    (\"cat\", categorical_pipe, cat)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "# 2) Fit & transform your training set\n",
        "X_tr_full = ct.fit_transform(X_train)\n",
        "\n",
        "# 3) Manually assemble the feature names:\n",
        "#    - numeric features keep their original names\n",
        "num_features = cont\n",
        "\n",
        "#    - categorical features come from the OneHotEncoder inside categorical_pipe\n",
        "cat_ohe_names = categorical_pipe.named_steps[\"onehot\"] \\\n",
        "                     .get_feature_names_out(cat)\n",
        "\n",
        "#    - combine\n",
        "feat_names = list(num_features) + list(cat_ohe_names)\n",
        "\n",
        "# 4) Report\n",
        "print(f\"➡️  Combined train matrix: {X_tr_full.shape[0]} rows × {X_tr_full.shape[1]} cols\")\n",
        "print(f\"📦  Total features = {len(feat_names)}\")\n",
        "print(\"🔍  Example feature names:\", feat_names[:10], \"…\", feat_names[-10:])\n",
        "\n",
        "# 5) (optional) display the first few rows in a DataFrame\n",
        "import pandas as pd\n",
        "df_feats = pd.DataFrame(X_tr_full, columns=feat_names, index=X_train.index)\n",
        "df_feats.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woGe7ZS8pl6a"
      },
      "outputs": [],
      "source": [
        "# ------------- 4.3 Define candidate models ------------------------------\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, cont),\n",
        "        (\"cat\", categorical_pipe, cat)\n",
        "    ],\n",
        "    remainder=\"drop\"   # or \"passthrough\" if you have other cols\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV0Ss9AWp-al"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 4.4 – helper for CatBoost (just fill categoricals) ═════════════╗\n",
        "def prepare_for_catboost(\n",
        "    X: pd.DataFrame,\n",
        "    cat_cols: list[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Copy X, replace any NaN in the specified categorical columns\n",
        "    with a literal 'Missing' string (so CatBoost sees it as a valid category).\n",
        "    Leave all other columns untouched (CatBoost will handle numeric NaNs if any).\n",
        "    \"\"\"\n",
        "    X_cb = X.copy()\n",
        "    for c in cat_cols:\n",
        "        X_cb[c] = X_cb[c].fillna(\"Missing\").astype(str)\n",
        "    return X_cb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-fk6MriImwl"
      },
      "outputs": [],
      "source": [
        "# # ╔═╡ Cell 4.5 — tune XGBoost rounds with native CV ═══════════════════════╗\n",
        "# import xgboost as xgb\n",
        "\n",
        "# # 1. Build DMatrix\n",
        "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "\n",
        "# # 2. Define a base params dict (everything except n_estimators)\n",
        "# xgb_params = {\n",
        "#     \"objective\":        \"binary:logistic\",\n",
        "#     \"eval_metric\":      \"auc\",\n",
        "#     \"learning_rate\":    0.01,\n",
        "#     \"max_depth\":        5,\n",
        "#     \"subsample\":        0.8,\n",
        "#     \"colsample_bytree\": 0.8,\n",
        "#     \"gamma\":            1.0,\n",
        "#     \"reg_alpha\":        0.1,\n",
        "#     \"reg_lambda\":       1.0,\n",
        "#     \"scale_pos_weight\": float((y_train==0).sum()) / (y_train==1).sum(),\n",
        "#     \"tree_method\":      \"hist\"   # or 'auto'\n",
        "# }\n",
        "\n",
        "# # 3. Run CV with early stopping\n",
        "# cv_results = xgb.cv(\n",
        "#     params=xgb_params,\n",
        "#     dtrain=dtrain,\n",
        "#     num_boost_round=2000,\n",
        "#     nfold=5,\n",
        "#     metrics=(\"auc\",),\n",
        "#     early_stopping_rounds=50,\n",
        "#     seed=RANDOM_STATE,\n",
        "#     as_pandas=True,\n",
        "#     verbose_eval=50\n",
        "# )\n",
        "\n",
        "# best_n_rounds = len(cv_results)\n",
        "# best_auc = cv_results[\"test-auc-mean\"].max()\n",
        "# print(f\"🔎 Optimal rounds: {best_n_rounds}, CV-AUC: {best_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io8XnbfaomCe"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 4.6 – Faster tune of ExtraTrees via RandomizedSearchCV with SMOTE ═╗\n",
        "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble       import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "\n",
        "# 1) Pipeline: impute/encode → SMOTE → ExtraTrees\n",
        "et_pipe = ImbPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"clf\", ExtraTreesClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=-1          # speed up individual fits\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2) Narrower/random search space\n",
        "param_dist = {\n",
        "    \"clf__n_estimators\":     [100, 300, 500, 800],\n",
        "    \"clf__max_depth\":        [None, 10, 20],\n",
        "    \"clf__max_features\":     [\"sqrt\", \"log2\"],\n",
        "    \"clf__min_samples_split\":[2, 5],\n",
        "    \"clf__min_samples_leaf\": [1, 2]\n",
        "}\n",
        "\n",
        "# 3) RandomizedSearchCV – just 30 draws instead of 720 fits\n",
        "et_rand = RandomizedSearchCV(\n",
        "    et_pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,             # only 30 random combinations\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# 4) Fit\n",
        "et_rand.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Best ExtraTrees params:\", et_rand.best_params_)\n",
        "print(\"✅ Best CV AUC:         \", et_rand.best_score_)\n",
        "\n",
        "# 5) Save the params\n",
        "best_et_params = {\n",
        "    \"n_estimators\":     et_rand.best_params_[\"clf__n_estimators\"],\n",
        "    \"max_depth\":        et_rand.best_params_[\"clf__max_depth\"],\n",
        "    \"max_features\":     et_rand.best_params_[\"clf__max_features\"],\n",
        "    \"min_samples_split\":et_rand.best_params_[\"clf__min_samples_split\"],\n",
        "    \"min_samples_leaf\": et_rand.best_params_[\"clf__min_samples_leaf\"]\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLC-7EkUo6Vm"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 4.8 – Randomized tune for HistGB with SMOTE ─═════════════════╗\n",
        "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble       import HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# 1) Pipeline: preprocess → SMOTE → HistGB\n",
        "hgb_pipe = ImbPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"clf\", HistGradientBoostingClassifier(\n",
        "        loss=\"log_loss\",\n",
        "        early_stopping=\"auto\",\n",
        "        validation_fraction=0.1,\n",
        "        n_iter_no_change=20,\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2) A richer space\n",
        "param_dist = {\n",
        "    \"clf__learning_rate\":      [0.005, 0.01, 0.03, 0.05, 0.1],\n",
        "    \"clf__max_iter\":           [200, 500, 1000, 1500],\n",
        "    \"clf__max_depth\":          [None, 3, 5, 7],\n",
        "    \"clf__min_samples_leaf\":   [20, 50, 100],\n",
        "    \"clf__l2_regularization\":  [0.0, 0.1, 1.0, 5.0],\n",
        "    \"clf__max_leaf_nodes\":     [15, 31, 63]\n",
        "}\n",
        "\n",
        "# 3) RandomizedSearchCV – 30 draws, 3-fold CV\n",
        "hgb_rand = RandomizedSearchCV(\n",
        "    hgb_pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# 4) Fit it\n",
        "hgb_rand.fit(X_train, y_train)\n",
        "\n",
        "# 5) Capture the winner\n",
        "best_hgb_params = {\n",
        "    k.replace(\"clf__\", \"\"): v\n",
        "    for k, v in hgb_rand.best_params_.items()\n",
        "}\n",
        "print(\"✅ Best HistGB params:\", best_hgb_params,\n",
        "      \"→ CV AUC:\", hgb_rand.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1B6O6HTjlM0"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 5 – Build models dict with SMOTE in every pipeline ─════════╗\n",
        "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "\n",
        "models = {\n",
        "    \"LogReg\": RandomizedSearchCV(\n",
        "        ImbPipeline([\n",
        "            (\"prep\", preprocessor),\n",
        "            (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "            (\"clf\", LogisticRegression(\n",
        "                solver=\"saga\", penalty=\"elasticnet\", l1_ratio=0.5,\n",
        "                class_weight=\"balanced\", max_iter=5000,\n",
        "                random_state=RANDOM_STATE\n",
        "            ))\n",
        "        ]),\n",
        "        param_distributions={\n",
        "            \"clf__C\":       [1e-3, 1e-2, 1e-1, 1, 10, 100],\n",
        "            \"clf__l1_ratio\":[0.0, 0.25, 0.5, 0.75, 1.0]\n",
        "        },\n",
        "        n_iter=20,\n",
        "        scoring=\"roc_auc\",\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        refit=True\n",
        "    ),\n",
        "\n",
        "    \"SVM\": RandomizedSearchCV(\n",
        "        ImbPipeline([\n",
        "            (\"prep\", preprocessor),\n",
        "            (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "            (\"clf\", SVC(\n",
        "                probability=True,\n",
        "                kernel=\"rbf\",\n",
        "                class_weight=\"balanced\",\n",
        "                random_state=RANDOM_STATE\n",
        "            ))\n",
        "        ]),\n",
        "        param_distributions={\n",
        "            \"clf__C\":     np.logspace(-3, 3, 20),\n",
        "            \"clf__gamma\": np.logspace(-4, 0, 20)\n",
        "        },\n",
        "        n_iter=30,\n",
        "        scoring=\"roc_auc\",\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        random_state=RANDOM_STATE,\n",
        "        refit=True\n",
        "    ),\n",
        "\n",
        "    \"NaiveBayes\": GridSearchCV(\n",
        "        ImbPipeline([\n",
        "            (\"prep\", preprocessor),\n",
        "            (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "            (\"clf\", GaussianNB())\n",
        "        ]),\n",
        "        param_grid={\"clf__var_smoothing\": np.logspace(-12, -6, 6)},\n",
        "        scoring=\"roc_auc\",\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        refit=True\n",
        "    ),\n",
        "\n",
        "    \"ExtraTrees\": ExtraTreesClassifier(\n",
        "        n_estimators      = best_et_n,\n",
        "        max_depth         = best_et_depth,\n",
        "        max_features      = best_et_feats,\n",
        "        min_samples_split = best_et_split,\n",
        "        min_samples_leaf  = best_et_leaf,\n",
        "        random_state      = RANDOM_STATE,\n",
        "        class_weight      = \"balanced\"\n",
        "    ),\n",
        "\n",
        "    \"HistGB\": HistGradientBoostingClassifier(\n",
        "        loss               = \"log_loss\",\n",
        "        learning_rate      = best_hgb_params[\"learning_rate\"],\n",
        "        max_iter           = best_hgb_params[\"max_iter\"],\n",
        "        max_depth          = best_hgb_params[\"max_depth\"],\n",
        "        min_samples_leaf   = best_hgb_params[\"min_samples_leaf\"],\n",
        "        l2_regularization  = best_hgb_params[\"l2_regularization\"],\n",
        "        max_leaf_nodes     = best_hgb_params[\"max_leaf_nodes\"],\n",
        "        early_stopping     = \"auto\",\n",
        "        validation_fraction= 0.1,\n",
        "        n_iter_no_change   = 20,\n",
        "        random_state       = RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "# rebuild cat_features_idx now that models dict is final\n",
        "cat_features_idx = [X_train.columns.get_loc(c) for c in cat]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvQoEpyqo8Ga"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 5.1 – CatBoost + SMOTE (fixed numeric imputer) ═══════════════╗\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# 1) Fully preprocess X_train → a NumPy matrix with no NaNs\n",
        "X_pre = preprocessor.fit_transform(X_train)\n",
        "assert not np.isnan(X_pre).any(), \"Still NaNs in X_pre!\"\n",
        "\n",
        "# 2) SMOTE once, on the whole matrix\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_res, y_res = sm.fit_resample(X_pre, y_train)\n",
        "\n",
        "# 3) Set up your CatBoost and param grid\n",
        "cb = CatBoostClassifier(\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"AUC\",\n",
        "    random_seed=RANDOM_STATE,\n",
        "    verbose=False\n",
        ")\n",
        "param_dist = {\n",
        "    \"iterations\":    [200, 500, 800],\n",
        "    \"depth\":         [6, 8],\n",
        "    \"learning_rate\": [0.03, 0.05],\n",
        "    \"l2_leaf_reg\":   [3, 5],\n",
        "}\n",
        "\n",
        "# 4) RandomizedSearchCV *on the resampled data*\n",
        "rs_cb = RandomizedSearchCV(\n",
        "    cb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True\n",
        ")\n",
        "rs_cb.fit(X_res, y_res)\n",
        "\n",
        "print(\"✅ Best CatBoost params:\", rs_cb.best_params_)\n",
        "print(\"✅ Best CV AUC:        \", rs_cb.best_score_)\n",
        "\n",
        "# 5) Finally wrap back into your pipeline so future .predict calls\n",
        "#    still apply the same preprocessor + best CatBoost model.\n",
        "from sklearn.pipeline import Pipeline as SkPipeline\n",
        "models[\"CatBoost\"] = SkPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"clf\",  rs_cb.best_estimator_)\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14rvNrWpouj3"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 5.2 – Tune AdaBoost n_estimators + learning_rate with SMOTE ─═╗\n",
        "from imblearn.pipeline       import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.tree            import DecisionTreeClassifier\n",
        "from sklearn.ensemble        import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Pipeline: preprocess → SMOTE → AdaBoost\n",
        "ab_pipe = ImbPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"clf\", AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE),\n",
        "        algorithm=\"SAMME\",\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Grid over AdaBoost parameters\n",
        "param_grid = {\n",
        "    \"clf__n_estimators\":  [100, 300, 500, 800],\n",
        "    \"clf__learning_rate\": [0.1, 0.25, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# 3-fold CV\n",
        "ab_grid = GridSearchCV(\n",
        "    ab_pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "ab_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Best AdaBoost params:\", ab_grid.best_params_, \"→ CV AUC:\", ab_grid.best_score_)\n",
        "best_ab_params = {\n",
        "    \"n_estimators\":  ab_grid.best_params_[\"clf__n_estimators\"],\n",
        "    \"learning_rate\": ab_grid.best_params_[\"clf__learning_rate\"]\n",
        "}\n",
        "\n",
        "# Final AdaBoost model\n",
        "models[\"AdaBoost\"] = ImbPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"clf\", AdaBoostClassifier(\n",
        "        estimator=DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE),\n",
        "        algorithm=\"SAMME\",\n",
        "        n_estimators=best_ab_params[\"n_estimators\"],\n",
        "        learning_rate=best_ab_params[\"learning_rate\"],\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_FbAFdsnjqz"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 5.3 – Randomized search → final XGBoost with SMOTE & early stopping ─╗\n",
        "import xgboost as xgb\n",
        "from imblearn.pipeline       import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Pipeline: preprocess → SMOTE → XGBClassifier (no early stopping yet)\n",
        "xgb_pipe = ImbPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"clf\", xgb.XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        use_label_encoder=False,\n",
        "        eval_metric=\"auc\",\n",
        "        tree_method=\"hist\",\n",
        "        scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Random grid\n",
        "param_dist = {\n",
        "    \"clf__n_estimators\":     [200, 500, 800, 1200],\n",
        "    \"clf__learning_rate\":    [0.005, 0.01, 0.03, 0.05],\n",
        "    \"clf__max_depth\":        [3, 5, 7, 9],\n",
        "    \"clf__subsample\":        [0.6, 0.8, 1.0],\n",
        "    \"clf__colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "    \"clf__gamma\":            [0, 0.1, 1.0],\n",
        "    \"clf__reg_alpha\":        [0, 0.1, 1.0],\n",
        "    \"clf__reg_lambda\":       [0.5, 1.0, 2.0]\n",
        "}\n",
        "\n",
        "rs_xgb = RandomizedSearchCV(\n",
        "    xgb_pipe,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring=\"roc_auc\",\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "rs_xgb.fit(X_train, y_train)\n",
        "\n",
        "print(\"✅ Best XGB params:\", rs_xgb.best_params_)\n",
        "print(\"✅ Best CV AUC:   \", rs_xgb.best_score_)\n",
        "\n",
        "# Extract best params and build final with early stopping\n",
        "best_params = {\n",
        "    key.replace(\"clf__\", \"\"): val\n",
        "    for key, val in rs_xgb.best_params_.items()\n",
        "}\n",
        "\n",
        "xgb_final = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    objective=\"binary:logistic\",\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"auc\",\n",
        "    tree_method=\"hist\",\n",
        "    scale_pos_weight=(y_train==0).sum()/(y_train==1).sum(),\n",
        "    early_stopping_rounds=30,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Pre-fit & transform once\n",
        "preprocessor.fit(X_train)\n",
        "X_tr_pre = preprocessor.transform(X_train)\n",
        "X_te_pre = preprocessor.transform(X_test)\n",
        "\n",
        "xgb_final.fit(\n",
        "    X_tr_pre, y_train,\n",
        "    eval_set=[(X_te_pre, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "models[\"XGBoost\"] = ImbPipeline([\n",
        "    (\"prep\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"clf\", xgb_final)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1p3iHnqjm3C"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 6 – 80/20 ShuffleSplit ROC (mean ± std), fixed XGBoost fit ═╗\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.pipeline       import Pipeline\n",
        "from sklearn.metrics        import roc_curve, roc_auc_score, RocCurveDisplay\n",
        "\n",
        "# 1) Fit the preprocessor once on your training set\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# 2) Recompute cat_features_idx on X_train\n",
        "cat_features_idx = [X_train.columns.get_loc(c) for c in cat]\n",
        "\n",
        "# 3) Define 5 independent 80/20 splits on the *training* set only\n",
        "ss = ShuffleSplit(n_splits=5, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "mean_fpr = np.linspace(0, 1, 200)\n",
        "aucs     = {name: [] for name in models}\n",
        "mean_tpr = {name: np.zeros_like(mean_fpr) for name in models}\n",
        "\n",
        "for tr_idx, val_idx in ss.split(X_train, y_train):\n",
        "    X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    # Pre-transform once per fold\n",
        "    X_tr_full  = preprocessor.transform(X_tr)\n",
        "    X_val_full = preprocessor.transform(X_val)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if name == \"CatBoost\":\n",
        "            X_tr_cb = prepare_for_catboost(X_tr, cat).astype(str)\n",
        "            X_val_cb = prepare_for_catboost(X_val, cat).astype(str)\n",
        "            model.fit(X_tr_cb, y_tr, cat_features=cat_features_idx, verbose=False)\n",
        "            y_prob = model.predict_proba(X_val_cb)[:, 1]\n",
        "\n",
        "        elif name == \"XGBoost\":\n",
        "            # ⚠️ Pull out the classifier and fit it directly\n",
        "            clf = model.named_steps[\"clf\"]\n",
        "            clf.fit(\n",
        "                X_tr_full, y_tr,\n",
        "                eval_set=[(X_val_full, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "            y_prob = clf.predict_proba(X_val_full)[:, 1]\n",
        "\n",
        "        elif name in (\"LogReg\", \"SVM\"):\n",
        "            model.fit(X_tr, y_tr)\n",
        "            y_prob = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        else:\n",
        "            pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", model)])\n",
        "            pipe.fit(X_tr, y_tr)\n",
        "            y_prob = pipe.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "        auc_val     = roc_auc_score(y_val, y_prob)\n",
        "\n",
        "        aucs[name].append(auc_val)\n",
        "        mean_tpr[name] += np.interp(mean_fpr, fpr, tpr)\n",
        "\n",
        "# 4) Build averaged RocCurveDisplay objects\n",
        "roc_displays = []\n",
        "for name in models:\n",
        "    mean_tpr[name] /= ss.get_n_splits()\n",
        "    mean_tpr[name][0]  = 0.0\n",
        "    mean_tpr[name][-1] = 1.0\n",
        "\n",
        "    auc_mean = np.mean(aucs[name])\n",
        "    auc_std  = np.std(aucs[name])\n",
        "\n",
        "    disp = RocCurveDisplay(\n",
        "        fpr=mean_fpr,\n",
        "        tpr=mean_tpr[name],\n",
        "        estimator_name=f\"{name} (AUC={auc_mean:.3f}±{auc_std:.3f})\"\n",
        "    )\n",
        "    roc_displays.append(disp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZiaxyy-joWW"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 7 ═════════════════════════════════════════════════════════════╗\n",
        "# ------------- 7. Plot combined ROC figure -----------------------------\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
        "\n",
        "for idx, disp in enumerate(roc_displays):\n",
        "    disp.plot(ax=plt.gca(), alpha=0.85, linewidth=2, color=colors[idx % len(colors)])\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"--\", color=\"grey\", linewidth=1)\n",
        "plt.title(\"ROC – Recurrence_2yr\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9AAvf6fRV0q"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 7b – Bootstrap AUC 95% CI ─────────────────────────────────╗\n",
        "import numpy as np\n",
        "from sklearn.metrics    import roc_auc_score\n",
        "\n",
        "# 1) ground truth\n",
        "y_true = y_test.reset_index(drop=True)\n",
        "\n",
        "# 2) pipeline handles preprocessing internally\n",
        "y_prob = models[\"XGBoost\"].predict_proba(\n",
        "    X_test.reset_index(drop=True)\n",
        ")[:, 1]\n",
        "\n",
        "# 3) bootstrap\n",
        "n_bootstraps = 1000\n",
        "rng = np.random.RandomState(42)\n",
        "scores = []\n",
        "\n",
        "for _ in range(n_bootstraps):\n",
        "    idx = rng.randint(0, len(y_true), len(y_true))\n",
        "    if len(np.unique(y_true.iloc[idx])) < 2:\n",
        "        continue\n",
        "    scores.append(roc_auc_score(y_true.iloc[idx], y_prob[idx]))\n",
        "\n",
        "ci_lower, ci_upper = np.percentile(scores, [2.5, 97.5])\n",
        "auc_mean = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "print(f\"AUC (test)         = {auc_mean:.3f}\")\n",
        "print(f\"95% CI (bootstrap)= [{ci_lower:.3f}, {ci_upper:.3f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Iwf0lehjq6_"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 8 – Bar chart of mean ShuffleSplit AUCs with ±std ─════════╗\n",
        "import numpy as np\n",
        "\n",
        "# 1. Compute mean & std per model\n",
        "mean_aucs = {name: np.mean(vals) for name, vals in aucs.items()}\n",
        "std_aucs  = {name: np.std(vals)  for name, vals in aucs.items()}\n",
        "\n",
        "# 2. Sort by mean AUC\n",
        "sorted_items = sorted(mean_aucs.items(), key=lambda kv: kv[1])\n",
        "labels, means = zip(*sorted_items)\n",
        "stds = [std_aucs[name] for name in labels]\n",
        "\n",
        "# 3. Plot bars with error bars\n",
        "plt.figure(figsize=(10, 4))\n",
        "bars = plt.bar(labels, means, yerr=stds, capsize=4)\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Mean AUC\")\n",
        "plt.title(\"Mean AUC ± Std – 80/20 ShuffleSplit (5 runs)\")\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "# 4. Annotate each bar with mean±std\n",
        "for bar, m, s in zip(bars, means, stds):\n",
        "    text = f\"{m:.3f}±{s:.3f}\"\n",
        "    plt.annotate(\n",
        "        text,\n",
        "        xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
        "        xytext=(0, 3), textcoords=\"offset points\",\n",
        "        ha=\"center\", va=\"bottom\", fontsize=8\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6w9OoFOjp4L"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 9 – Quick summary table with mean ± std AUC ─════════════╗\n",
        "import numpy as np\n",
        "\n",
        "# Build a list of records\n",
        "records = []\n",
        "for name, vals in aucs.items():\n",
        "    records.append({\n",
        "        \"Model\":    name,\n",
        "        \"Mean AUC\": np.mean(vals),\n",
        "        \"Std AUC\":  np.std(vals)\n",
        "    })\n",
        "\n",
        "# Create DataFrame and sort\n",
        "summary = (\n",
        "    pd.DataFrame(records)\n",
        "      .sort_values(\"Mean AUC\", ascending=False)\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Plain‐text print\n",
        "print(summary.to_string(index=False, float_format=\"%.3f\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY9chGwXLmvL"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 9.5′ – Pipeline + sigmoid calibration ─────────────────────╗\n",
        "from sklearn.pipeline            import Pipeline\n",
        "from sklearn.calibration        import CalibratedClassifierCV\n",
        "from xgboost                    import XGBClassifier\n",
        "\n",
        "# 1) Rebuild a pipeline: preprocessing → XGBoost\n",
        "xgb_pipe = Pipeline([\n",
        "    (\"prep\", preprocessor),      # your ColumnTransformer!\n",
        "    (\"clf\", XGBClassifier(\n",
        "        n_estimators       = best_n_rounds,\n",
        "        learning_rate      = 0.01,\n",
        "        max_depth          = 5,\n",
        "        subsample          = 0.8,\n",
        "        colsample_bytree   = 0.8,\n",
        "        gamma              = 1.0,\n",
        "        reg_alpha          = 0.1,\n",
        "        reg_lambda         = 1.0,\n",
        "        scale_pos_weight   = (y_train==0).sum()/(y_train==1).sum(),\n",
        "        objective          = \"binary:logistic\",\n",
        "        eval_metric        = \"auc\",\n",
        "        tree_method        = \"hist\",\n",
        "        use_label_encoder  = False,\n",
        "        random_state       = RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2) Wrap that in a sigmoid calibrator (Platt scaling) with 3-fold CV\n",
        "calibrator = CalibratedClassifierCV(\n",
        "    base_estimator = xgb_pipe,\n",
        "    method         = \"sigmoid\",   # more stable on small samples\n",
        "    cv             = 3            # internal 3-fold for calibration\n",
        ")\n",
        "\n",
        "# 3) Fit _only_ on your training set\n",
        "calibrator.fit(X_train, y_train)\n",
        "\n",
        "# 4) Save back into your models dict\n",
        "models[\"XGBoost_Calibrated\"] = calibrator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSquBqE3I32F"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 10 – Calibration curve + Brier score ─────────────────══════╗\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics      import brier_score_loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Raw probabilities (before calibration) on the raw X_test\n",
        "y_prob_raw = models[\"XGBoost\"].predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 2) Calibrated probabilities (same raw X_test)\n",
        "y_prob_cal = models[\"XGBoost_Calibrated\"].predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 3) Compute calibration curves (fraction of positives vs. mean predicted)\n",
        "frac_pos_raw, mean_pred_raw = calibration_curve(y_test, y_prob_raw, n_bins=10)\n",
        "frac_pos_cal, mean_pred_cal = calibration_curve(y_test, y_prob_cal, n_bins=10)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(mean_pred_raw, frac_pos_raw, \"s-\", label=\"XGBoost (raw)\")\n",
        "plt.plot(mean_pred_cal, frac_pos_cal, \"o-\", label=\"XGBoost (calibrated)\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n",
        "plt.xlabel(\"Mean predicted probability\")\n",
        "plt.ylabel(\"Fraction of positives\")\n",
        "plt.title(\"Calibration curve – XGBoost\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4) Brier scores for raw vs. calibrated\n",
        "print(\"Brier score (raw)     :\", brier_score_loss(y_test, y_prob_raw))\n",
        "print(\"Brier score (calibr.) :\", brier_score_loss(y_test, y_prob_cal))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bz5CwvWJBRd"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 11 – Precision–Recall & threshold‐tuning ════════════════╗\n",
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "y_prob = models[\"XGBoost\"].predict_proba(X_test)[:,1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(recall, precision, label=\"XGBoost PR-curve\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision–Recall curve\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# F1 vs threshold\n",
        "f1s = [f1_score(y_test, y_prob>=t) for t in thresholds]\n",
        "opt_idx = np.argmax(f1s)\n",
        "print(f\"Best F1={f1s[opt_idx]:.3f} at threshold={thresholds[opt_idx]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V_vAecBU8TX"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 11 – SHAP summary plots for XGBoost ─══════════════════════╗\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Build a SHAP Explainer on your training set\n",
        "explainer = shap.Explainer(models[\"XGBoost\"], X_train)\n",
        "\n",
        "# 2) Compute SHAP values on the test set: this returns an Explanation object\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# 3) Beeswarm (dot) plot of per‐sample SHAP values\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.plots.beeswarm(shap_values, max_display=20, show=False)\n",
        "plt.title(\"SHAP Beeswarm – XGBoost (Recurrence_2yr)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4) Bar plot of mean(|SHAP|) for each feature\n",
        "plt.figure(figsize=(6, 8))\n",
        "shap.plots.bar(shap_values, max_display=20, show=False)\n",
        "plt.title(\"mean(|SHAP|) – XGBoost\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1smZQNICJKPL"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 12 – Partial dependence for top features ═══════════════╗\n",
        "import numpy as np\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Extract the raw SHAP values array from the Explanation\n",
        "#    shap_values was created in Cell 10 via `explainer(X_test)`\n",
        "vals = shap_values.values\n",
        "# If it’s a 3‐D array (n_samples x 2 classes x n_features),\n",
        "# pick the positive‐class slice:\n",
        "if vals.ndim == 3:\n",
        "    vals = vals[:, 1, :]\n",
        "\n",
        "# 2) Compute mean(|SHAP|) per feature and grab top-3 indices\n",
        "importances = np.abs(vals).mean(axis=0)\n",
        "top_idx    = np.argsort(importances)[::-1][:3]\n",
        "top_feats  = [X_test.columns[i] for i in top_idx]\n",
        "\n",
        "print(\"Top 3 features by |SHAP|:\", top_feats)\n",
        "\n",
        "# 3) Plot partial dependence for those top 3\n",
        "fig, axes = plt.subplots(1, len(top_feats), figsize=(4*len(top_feats), 4))\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    models[\"XGBoost\"],\n",
        "    X_test,\n",
        "    features=top_feats,\n",
        "    ax=axes\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blQhd6RJJOxU"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 13 – Permutation importance ══════════════════════════╗\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "r = permutation_importance(\n",
        "    models[\"XGBoost\"], X_test, y_test,\n",
        "    scoring=\"roc_auc\", n_repeats=10, random_state=RANDOM_STATE\n",
        ")\n",
        "perm_df = pd.Series(r.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
        "perm_df.head(10).plot.barh()\n",
        "plt.xlabel(\"Mean decrease in AUC\")\n",
        "plt.title(\"Permutation importance – XGBoost\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WVg9QelJVSw"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 14 – Manual Decision Curve Analysis (Net Benefit) ─════════════╗\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) True labels & predicted probabilities (from your best model)\n",
        "y_true = y_test.values        # shape (n_samples,)\n",
        "y_prob = models[\"XGBoost\"].predict_proba(X_test)[:,1]\n",
        "\n",
        "N      = len(y_true)\n",
        "preval = y_true.mean()        # event rate\n",
        "\n",
        "# 2) Define a grid of thresholds\n",
        "thresholds = np.linspace(0.01, 0.99, 99)\n",
        "\n",
        "# 3) Compute net benefit for each threshold\n",
        "nb_model = []\n",
        "nb_all   = []\n",
        "nb_none  = np.zeros_like(thresholds)\n",
        "\n",
        "for pt in thresholds:\n",
        "    preds = (y_prob >= pt).astype(int)\n",
        "    TP    = ((preds == 1) & (y_true == 1)).sum()\n",
        "    FP    = ((preds == 1) & (y_true == 0)).sum()\n",
        "    # NB = TP/N - FP/N * (pt/(1-pt))\n",
        "    nb    = TP/N - FP/N * (pt/(1-pt))\n",
        "    nb_model.append(nb)\n",
        "    nb_all.append(preval - (1 - preval)*(pt/(1-pt)))\n",
        "\n",
        "# 4) Plot\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(thresholds, nb_model, label=\"XGBoost\")\n",
        "plt.plot(thresholds, nb_all,   label=\"Treat All\",    linestyle=\"--\")\n",
        "plt.plot(thresholds, nb_none,  label=\"Treat None\",   linestyle=\":\")\n",
        "plt.xlabel(\"Decision Threshold\")\n",
        "plt.ylabel(\"Net Benefit\")\n",
        "plt.title(\"Decision Curve Analysis – Recurrence_2yr\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-7GFSRefJ0e"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 15 – SHAP summary for CatBoost (Recurrence_2yr) ─════════════╗\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Prepare CatBoost-friendly data\n",
        "X_test_cb = prepare_for_catboost(X_test, cat)\n",
        "\n",
        "# 2) Build a TreeExplainer that can handle your string categories\n",
        "explainer_cb = shap.TreeExplainer(\n",
        "    models[\"CatBoost\"],\n",
        "    feature_perturbation=\"tree_path_dependent\"\n",
        ")\n",
        "\n",
        "# 3) Compute the full SHAP matrix for the positive/recurrence class\n",
        "shap_values_cb = explainer_cb.shap_values(X_test_cb)\n",
        "# shap_values_cb should now be shape (n_samples, n_features)\n",
        "\n",
        "print(\"🔎 SHAP values shape:\", shap_values_cb.shape)\n",
        "\n",
        "# 4) Beeswarm (dot) plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "shap.summary_plot(\n",
        "    shap_values_cb,    # the full matrix, not a single row\n",
        "    X_test_cb,\n",
        "    max_display=20,\n",
        "    show=False\n",
        ")\n",
        "plt.title(\"SHAP Beeswarm – CatBoost (Recurrence_2yr)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5) Bar plot of mean(|SHAP|) global importances\n",
        "plt.figure(figsize=(6, 8))\n",
        "shap.summary_plot(\n",
        "    shap_values_cb,\n",
        "    X_test_cb,\n",
        "    plot_type=\"bar\",\n",
        "    max_display=20,\n",
        "    show=False\n",
        ")\n",
        "plt.title(\"mean(|SHAP|) – CatBoost\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_pdzzce-T8p"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 16 – Partial dependence (top 3 CatBoost features) ═════════╗\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "import numpy as np\n",
        "\n",
        "# pick the top 3 by mean(|SHAP|)\n",
        "top3 = X_test_cb.columns[np.argsort(np.abs(shap_values_cb).mean(0))[-3:]]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(4*3, 4))\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    models[\"CatBoost\"],      # your final CatBoost instance\n",
        "    X_test_cb,               # preprocessed test set\n",
        "    features=list(top3),     # names of the top 3\n",
        "    ax=axes\n",
        ")\n",
        "plt.suptitle(\"Partial Dependence – CatBoost\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkYcNh6e-VfF"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 17 – Permutation importance (CatBoost) ═════════════════════╗\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "r = permutation_importance(\n",
        "    models[\"CatBoost\"],\n",
        "    X_test_cb, y_test,\n",
        "    scoring=\"roc_auc\",\n",
        "    n_repeats=20,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# sort and plot\n",
        "import pandas as pd\n",
        "imp_df = pd.DataFrame({\n",
        "    \"feature\": X_test_cb.columns,\n",
        "    \"mean_decrease_auc\": r.importances_mean\n",
        "}).sort_values(\"mean_decrease_auc\", ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(6,8))\n",
        "plt.barh(imp_df[\"feature\"][::-1], imp_df[\"mean_decrease_auc\"][::-1])\n",
        "plt.xlabel(\"Mean decrease in AUC\")\n",
        "plt.title(\"Permutation importance – CatBoost\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvyUWuG0-WO5"
      },
      "outputs": [],
      "source": [
        "# ╔═╡ Cell 18 – Decision curve (CatBoost) ─────────────────────────────╗\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# (you can reuse your decision‐curve function or library)\n",
        "# here’s a very simple “net benefit” curve for illustration:\n",
        "probs = models[\"CatBoost\"].predict_proba(X_test_cb)[:,1]\n",
        "thresholds = np.linspace(0,1,100)\n",
        "n = len(y_test)\n",
        "event_rate = y_test.mean()\n",
        "\n",
        "nb = []\n",
        "for t in thresholds:\n",
        "    tp = ((probs >= t) & (y_test==1)).sum()\n",
        "    fp = ((probs >= t) & (y_test==0)).sum()\n",
        "    nb.append((tp/n) - (fp/n)*(t/(1-t)))  # simple net‐benefit formula\n",
        "\n",
        "plt.plot(thresholds, nb, label=\"CatBoost\")\n",
        "plt.plot(thresholds, np.zeros_like(thresholds), '--', label=\"Treat none\")\n",
        "plt.plot(thresholds,\n",
        "         event_rate - (1-event_rate)*(thresholds/(1-thresholds)),\n",
        "         ':', label=\"Treat all\")\n",
        "plt.xlabel(\"Decision threshold\")\n",
        "plt.ylabel(\"Net benefit\")\n",
        "plt.title(\"Decision Curve Analysis – CatBoost\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMxPbZYqGSkU5exJhLw/uRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}